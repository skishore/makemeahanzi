{
  "name": "Make Me a Hanzi",
  "tagline": "Free, open-source Chinese character data",
  "body": "![Make Me a Hanzi annotation tool](example.gif)\r\n\r\n### [Make Me a Hanzi Demo](https://makemeahanzi.herokuapp.com)\r\n\r\n### [New: Inkstone Chinese writing app](https://skishore.github.io/inkstone/)\r\n\r\nMake Me a Hanzi provides dictionary and graphical data for over 9000 of the\r\nmost common simplified and traditional Chinese characters. Among other things,\r\nthis data includes stroke-order vector graphics for all these characters. You\r\ncan see the project output at the [demo site](https://makemeahanzi.herokuapp.com)\r\nwhere you can look up a characters by drawing them. You can also download the\r\ndata for use in your own site or app.\r\n\r\nSee the [project site](http://skishore.github.io/makemeahanzi) for general\r\ninformation and updates on the project.\r\n\r\nMake Me a Hanzi data is split into two data files,\r\n[dictionary.txt](https://github.com/skishore/makemeahanzi/blob/master/dictionary.txt)\r\nand [graphics.txt](https://github.com/skishore/makemeahanzi/blob/master/graphics.txt),\r\nbecause the sources that the files are derived from have different licenses.\r\nIn addition, we provide an experimental tarball of animated SVGs,\r\n[svgs.tar.gz](https://github.com/skishore/makemeahanzi/blob/master/svgs.tar.gz)\r\nthat is licensed the same way as graphics.txt.\r\nSee the Sources section and the\r\n[COPYING](https://github.com/skishore/makemeahanzi/blob/master/COPYING)\r\nfile for more information.\r\n\r\n### Sources\r\n\r\n- dictionary.txt is derived from data from\r\n  [Unihan](http://unicode.org/charts/unihan.html)\r\n  and [CJKlib](https://github.com/cburgmer/cjklib).\r\n\r\n- graphics.txt and svgs.tar.gz are derived from two free fonts:\r\n  [Arphic PL KaitiM GB](https://apps.ubuntu.com/cat/applications/precise/fonts-arphic-gkai00mp/)\r\n  and [Arphic PL UKai](https://apps.ubuntu.com/cat/applications/fonts-arphic-ukai/).\r\n\r\nThis project would not have been possible without the generosity of\r\n[Arphic Technology](http://www.arphic.com/), a Taiwanese font forge that\r\nreleased their work under a permissive license in 1999.\r\n\r\nIn addition, I would like to thank Gábor Ugray for his thoughtful advice on\r\nthe project and for verifying stroke data for most of the traditional\r\ncharacters in the two data sets. Gábor maintains [Zydeo](http://zydeo.net/),\r\na free and open-source Chinese dictionary.\r\n\r\n### Format\r\n\r\nBoth dictionary.txt and graphics.txt are '\\n'-separated lists of lines, where\r\neach line is JSON object. They differ in which keys are present, but the\r\ncommon key, 'character', can be used to join the two data sets. You can also\r\nrely on the fact that the two files will always come in the same order.\r\n\r\n#### dictionary.txt keys:\r\n\r\n- __character:__ The Unicode character for this glyph. Required.\r\n\r\n- __definition:__ A String definition targeted towards second-language\r\n  learners. Optional.\r\n\r\n- __pinyin__ A comma-separated list of String pronunciations of this character.\r\n  Required, but may be empty.\r\n\r\n- __decomposition:__ An [Ideograph Description Sequence](https://en.wikipedia.org/wiki/Chinese_character_description_languages#Ideographic_Description_Sequences)\r\n  decomposition of the character. Required, but invalid if it starts with a\r\n  full-width question mark '？'.\r\n\r\n    Note that even if the first character is a\r\n    proper IDS symbol, any component within the decomposition may be a wide\r\n    question mark as well. For example, if we have a decomposition of a\r\n    character into a top and bottom component but can only recognize the top\r\n    component, we might have a decomposition like so: '⿱逢？'\r\n\r\n- __etymology:__ An etymology for the character. This field may be null. If\r\n  present, it will always have a \"type\" field, which will be one of\r\n  \"ideographic\", \"pictographic\", or \"pictophonetic\".\r\n  If the type is one of the first two options, then the etymology will\r\n  always include a string \"hint\" field explaining its formation.\r\n\r\n  If the type is \"pictophonetic\", then the etymology will contain three\r\n  other fields: \"hint\", \"phonetic\", and \"semantic\", each of which is\r\n  a string and each of which may be null. The etymology should be read as:\r\n      ${semantic} (${hint}) provides the meaning while ${phonetic}\r\n      provides the pronunciation.\r\n  with allowances for possible null values.\r\n\r\n- __radical:__ Unicode primary radical for this character. Required.\r\n\r\n- __matches:__\r\n  A list of mappings from strokes of this character to strokes of its\r\n  components, as indexed in its decomposition tree. Any given entry in\r\n  this list may be null. If an entry is not null, it will be a list of\r\n  indices corresponding to a path down the decomposition tree.\r\n\r\n  This schema is a little tricky to explain without an example. Suppose\r\n  that the character '俢' has the decomposition: '⿰亻⿱夂彡'\r\n\r\n  The third stroke in that character belongs to the radical '夂'.\r\n  Its match would be [1, 0]. That is, if you think of the decomposition as\r\n  a tree, it has '⿰' at its root with two children '亻' and '⿱', and\r\n  '⿱' further has two children '夂' and '彡'. The path down the tree\r\n  to '夂' is to take the second child of '⿰' and the first of '⿱',\r\n  hence, [1, 0].\r\n\r\n  This field can be used to generate visualizations marking each component\r\n  within a given character, or potentially for more exotic purposes.\r\n\r\n#### graphics.txt keys:\r\n\r\n- __character:__ The Unicode character for this glyph. Required.\r\n\r\n- __strokes:__\r\n  List of SVG path data for each stroke of this character, ordered by\r\n  proper stroke order. Each stroke is laid out on a 1024x1024 size\r\n  coordinate system where:\r\n    - The upper-left corner is at position (0, 900).\r\n    - The lower-right corner is at position (1024, -124).\r\n\r\n  Note that the y-axes DECREASES as you move downwards, which is strage!\r\n  To display these paths properly, you should hide render them as follows:\r\n\r\n      <svg viewBox=\"0 0 1024 1024\">\r\n        <g transform=\"scale(1, -1) translate(0, -900)\">\r\n          <path d=\"STROKE[0] DATA GOES HERE\"></path>\r\n          <path d=\"STROKE[1] DATA GOES HERE\"></path>\r\n          ...\r\n        </g>\r\n      </svg>\r\n\r\n- __medians:__\r\n  A list of stroke medians, in the same coordinate system as the SVG\r\n  paths above. These medians can be used to produce a rough stroke-order\r\n  animation, although it is a bit tricky. Each median is a list of pairs\r\n  of integers. This list will be as long as the strokes list.\r\n\r\n### TODOs and Future Work\r\n\r\n- Right now, all stroke order information is based on the People's Republic\r\n  of China (PRC) stroke order. Some characters are written with different\r\n  stroke orders in Japan, Taiwan, and elsewhere. We should build data for\r\n  these orders as well.\r\n\r\n- As an experimental next step, we have produced an animated SVG image for\r\n  each character that we have data for (see the svgs directory). It's easy to\r\n  It's easy to embed these SVGs in a website. A minimal example is as follows:\r\n\r\n      <body><embed src=\"31119.svg\" width=\"200px\" height=\"200px\"/></body>\r\n\r\n  This feature is experimental because it is still tricky to work with these\r\n  images beyond this basic example. For instance, it's not clear how to\r\n  embed two of these images side-by-side and have the second start animating\r\n  when the first is complete. However, the images are still the easiest way\r\n  to make use of this data..\r\n\r\nThese TODOs will be addressed based on prospective clients' needs, so if you\r\nwant something done, please let me know!\r\n",
  "google": "UA-73621002-1",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}